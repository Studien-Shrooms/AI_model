{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-06T08:34:46.193558Z",
     "iopub.status.busy": "2025-04-06T08:34:46.193266Z",
     "iopub.status.idle": "2025-04-06T08:35:00.325641Z",
     "shell.execute_reply": "2025-04-06T08:35:00.324739Z",
     "shell.execute_reply.started": "2025-04-06T08:34:46.193536Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -U transformers accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vison model form input in kaggel laden "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T08:35:00.327437Z",
     "iopub.status.busy": "2025-04-06T08:35:00.327142Z",
     "iopub.status.idle": "2025-04-06T08:39:41.656213Z",
     "shell.execute_reply": "2025-04-06T08:39:41.655575Z",
     "shell.execute_reply.started": "2025-04-06T08:35:00.327390Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09f5b4c340dc44daba24fe863b517195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# import torch\n",
    "# from transformers import MllamaForConditionalGeneration, AutoProcessor\n",
    "\n",
    "# base_model = \"/kaggle/input/llama-3.2-vision/transformers/11b-vision-instruct/1\"\n",
    "\n",
    "# processor = AutoProcessor.from_pretrained(base_model)\n",
    "\n",
    "# model = MllamaForConditionalGeneration.from_pretrained(\n",
    "#     base_model,\n",
    "#     low_cpu_mem_usage=True,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map=\"auto\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testin basic unfintuedned model with thes mp4 to see performace bevore finetuneing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T09:01:09.868617Z",
     "iopub.status.busy": "2025-04-06T09:01:09.868293Z",
     "iopub.status.idle": "2025-04-06T09:03:48.737535Z",
     "shell.execute_reply": "2025-04-06T09:03:48.736749Z",
     "shell.execute_reply.started": "2025-04-06T09:01:09.868592Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50b2c21eb103457cb4b2395dffd0cea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "\n",
      "Was sagt die Person in Gebärdensprache? Antworte in Hochdeutsch.assistant\n",
      "\n",
      "Die Person in Gebärdensprache sagt: \"Ich liebe dich.\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import requests\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "from transformers import MllamaForConditionalGeneration, AutoProcessor\n",
    "\n",
    "# === Modellpfad & Laden ===\n",
    "base_model = \"/kaggle/input/llama-3.2-vision/transformers/11b-vision-instruct/1\"\n",
    "processor = AutoProcessor.from_pretrained(base_model)\n",
    "model = MllamaForConditionalGeneration.from_pretrained(\n",
    "    base_model,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Clear CUDA cache before starting\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# === 1. Lade lokalen Video-Frame ===\n",
    "def extract_frame_from_video(video_path, frame_number=0):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "    if not ret:\n",
    "        raise ValueError(\"Kein Frame gefunden.\")\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    return Image.fromarray(image)\n",
    "\n",
    "video_path = \"/kaggle/input/testingthemodel/test_file_DGL.mp4\"  # ← Pfad zu deiner Datei!\n",
    "image = extract_frame_from_video(video_path, frame_number=10)  # z.B. Frame 10\n",
    "\n",
    "# Optional verkleinern, um Speicher zu sparen\n",
    "image = image.resize((224, 224))\n",
    "\n",
    "# === 2. Prompt bauen ===\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": \"Analysiere die Bildsequenz. Sie besteht aus mehreren Einzelbildern, die aus einem Video extrahiert wurden.  Die Bilder zeigen die zeitliche Entwicklung einer Gebärde, angeordnet von links nach rechts. Gib den gesprochenen Inhalt in Hochdeutsch zurück.\"}\n",
    "    ]}\n",
    "]\n",
    "input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "\n",
    "# === 3. Verarbeitung vorbereiten ===\n",
    "inputs = processor(image, input_text, return_tensors=\"pt\").to(model.device)\n",
    "inputs = {k: v.to(dtype=torch.float16) if v.dtype == torch.float32 else v for k, v in inputs.items()}\n",
    "\n",
    "# === 4. Modell starten ===\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, max_new_tokens=60)\n",
    "\n",
    "# === 5. Ausgabe anzeigen ===\n",
    "print(processor.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-06T08:39:42.933950Z",
     "iopub.status.idle": "2025-04-06T08:39:42.934356Z",
     "shell.execute_reply": "2025-04-06T08:39:42.934174Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-06T08:39:42.935176Z",
     "iopub.status.idle": "2025-04-06T08:39:42.935575Z",
     "shell.execute_reply": "2025-04-06T08:39:42.935390Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7062721,
     "sourceId": 11295124,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 121030,
     "modelInstanceId": 100923,
     "sourceId": 119991,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

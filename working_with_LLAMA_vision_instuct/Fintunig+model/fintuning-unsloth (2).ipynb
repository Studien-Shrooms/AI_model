{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11312317,"sourceType":"datasetVersion","datasetId":7075357}],"dockerImageVersionId":30919,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install pip3-autoremove\n!pip-autoremove torch torchvision torchaudio -y\n!pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu121\n!pip install unsloth\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T14:33:39.700087Z","iopub.execute_input":"2025-04-07T14:33:39.700364Z","iopub.status.idle":"2025-04-07T14:37:54.887881Z","shell.execute_reply.started":"2025-04-07T14:33:39.700333Z","shell.execute_reply":"2025-04-07T14:37:54.886842Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from unsloth import FastVisionModel\nmodel, tokenizer = FastVisionModel.from_pretrained(\n    \"unsloth/Llama-3.2-11B-Vision-Instruct\",\n    load_in_4bit = True,\n    use_gradient_checkpointing = \"unsloth\",\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T14:37:54.891803Z","iopub.execute_input":"2025-04-07T14:37:54.892133Z","iopub.status.idle":"2025-04-07T14:39:20.510122Z","shell.execute_reply.started":"2025-04-07T14:37:54.892098Z","shell.execute_reply":"2025-04-07T14:39:20.509099Z"}},"outputs":[{"name":"stdout","text":"ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\nUnsloth: Failed to patch Gemma3ForConditionalGeneration.\nü¶• Unsloth Zoo will now patch everything to make training faster!\n==((====))==  Unsloth 2025.3.19: Fast Mllama patching. Transformers: 4.51.0.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/375k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61f5a10928444677a11db46569e3630f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53c35ef179aa49afbd72ea22705f6291"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/2.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d42e84aa20b54b9d801f18a31a126690"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6a8b7a52a8d4ac3a11bc039109b6f34"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/210 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3001386bbed347bf899b453091f46dd5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/477 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad81f3b8579c4e1587f38a18f5e7ede6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/55.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bea624507eda4ee98d19d40df60a4682"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f288472906b43ebac8b1f1440d156e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef3852f09c4c428f8eb0392f17e1e005"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.json:   0%|          | 0.00/5.15k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"499aabf0a0724514b50b402fa9f7fabb"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"model = FastVisionModel.get_peft_model(\n    model,\n    finetune_vision_layers = True,\n    finetune_language_layers = True,\n    finetune_attention_modules = True,\n    finetune_mlp_modules = True,\n    r = 16,\n    lora_alpha = 16,\n    lora_dropout = 0,\n    bias = \"none\",\n    random_state = 3407,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T14:42:07.019007Z","iopub.execute_input":"2025-04-07T14:42:07.019459Z","iopub.status.idle":"2025-04-07T14:42:13.470357Z","shell.execute_reply.started":"2025-04-07T14:42:07.019415Z","shell.execute_reply":"2025-04-07T14:42:13.469226Z"}},"outputs":[{"name":"stdout","text":"Unsloth: Making `model.base_model.model.vision_model.transformer` require gradients\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import json\nfrom PIL import Image\nfrom datasets import Dataset\n\n# JSON laden\nwith open(\"/kaggle/input/finetune/fintuntin_small/fintunging.json\", \"r\") as f:\n    data = json.load(f)\n\n# Bild laden und hinzuf√ºgen\nfor item in data:\n    item[\"image\"] = Image.open(item[\"image\"]).convert(\"RGB\")\n\n# In Huggingface Dataset umwandeln\ndataset = Dataset.from_list(data)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T14:45:06.718385Z","iopub.execute_input":"2025-04-07T14:45:06.718729Z","iopub.status.idle":"2025-04-07T14:45:06.958905Z","shell.execute_reply.started":"2025-04-07T14:45:06.718684Z","shell.execute_reply":"2025-04-07T14:45:06.957761Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-906cd8e2c7bc>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Bild laden und hinzuf√ºgen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# In Huggingface Dataset umwandeln\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3468\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3469\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3470\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3471\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/finetune/fintuntin_small/Alltag_collage.png'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/finetune/fintuntin_small/Alltag_collage.png'","output_type":"error"}],"execution_count":4},{"cell_type":"code","source":"instruction = \"Analysiere die Bildsequenz. Sie besteht aus mehreren Einzelbildern, die aus einem Video extrahiert wurden.  Die Bilder zeigen die zeitliche Entwicklung einer Geb√§rde, angeordnet von links nach rechts. Gib den gesprochenen Inhalt in Hochdeutsch zur√ºck.\"\n\ndef convert_to_conversation(sample):\n    return {\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": instruction},\n                    {\"type\": \"image\", \"image\": sample[\"image\"]}\n                ]\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": [{\"type\": \"text\", \"text\": sample[\"label\"]}]\n            },\n        ]\n    }\n\nconverted_dataset = [convert_to_conversation(row) for row in dataset]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from unsloth.trainer import UnslothVisionDataCollator\nfrom trl import SFTTrainer, SFTConfig\nfrom unsloth import is_bf16_supported\n\nFastVisionModel.for_training(model)\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    data_collator=UnslothVisionDataCollator(model, tokenizer),\n    train_dataset=converted_dataset,\n    args=SFTConfig(\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        max_steps=30,\n        learning_rate=2e-4,\n        fp16=not is_bf16_supported(),\n        bf16=is_bf16_supported(),\n        logging_steps=1,\n        output_dir=\"outputs\",\n        remove_unused_columns=False,\n        dataset_text_field=\"\",\n        dataset_kwargs={\"skip_prepare_dataset\": True},\n        dataset_num_proc=4,\n        max_seq_length=2048,\n    ),\n)\n\ntrainer.train()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained(\"lora_model\")\ntokenizer.save_pretrained(\"lora_model\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"FastVisionModel.for_inference(model)\n\nimage = Image.open(\"/kaggle/input/finetune/fintuntin_small/fragen_collage.png\").convert(\"RGB\")\ninstruction = \"Analysiere die Bildsequenz. Sie besteht aus mehreren Einzelbildern, die aus einem Video extrahiert wurden.  Die Bilder zeigen die zeitliche Entwicklung einer Geb√§rde, angeordnet von links nach rechts. Gib den gesprochenen Inhalt in Hochdeutsch zur√ºck.\"\n\nmessages = [\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"image\", \"image\": image},\n        {\"type\": \"text\", \"text\": instruction}\n    ]}\n]\n\ninput_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\ninputs = tokenizer(image, input_text, add_special_tokens=False, return_tensors=\"pt\").to(\"cuda\")\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer, skip_prompt=True)\n\n_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=128, temperature=1.5, min_p=0.1)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}